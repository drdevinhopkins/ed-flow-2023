{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf00d2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhopkins/apps/ed-flow-2023/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chronos import BaseChronosPipeline, Chronos2Pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from utils import upload\n",
    "import dropbox\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "import holidays\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5ddd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(7)\n",
    "\n",
    "# Load the Chronos-2 pipeline\n",
    "# GPU recommended for faster inference, but CPU is also supported\n",
    "pipeline: Chronos2Pipeline = BaseChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-2\",\n",
    "    # device_map=\"cuda\"\n",
    "    device_map=\"cuda\",\n",
    "    force_download=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00df3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def regularize_hourly(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reindex each group's timestamps to strict hourly and fill gaps.\n",
    "    Works whether the grouping column is present or omitted (include_groups=False).\n",
    "    \"\"\"\n",
    "    # The group key (id) is available as g.name; if ID_COL exists, prefer it.\n",
    "    sid = g[ID_COL].iloc[0] if ID_COL in g.columns else g.name\n",
    "\n",
    "    g = g.sort_values(TS_COL)\n",
    "    full_idx = pd.date_range(g[TS_COL].min(), g[TS_COL].max(), freq=\"h\")\n",
    "    g = g.set_index(TS_COL).reindex(full_idx)\n",
    "    g.index.name = TS_COL\n",
    "\n",
    "    # restore id (constant for the whole group)\n",
    "    g[ID_COL] = sid\n",
    "\n",
    "    # numeric + fill for targets\n",
    "    for col in TARGETS:\n",
    "        if col in g.columns:\n",
    "            g[col] = pd.to_numeric(g[col], errors=\"coerce\").ffill().bfill()\n",
    "    return g.reset_index()\n",
    "\n",
    "def add_holiday_flags(\n",
    "    df: pd.DataFrame,\n",
    "    ts_col: str = \"ds\",\n",
    "    local_tz: str = \"America/Montreal\",\n",
    "    observed: bool = True,\n",
    "    include_names: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds boolean columns:\n",
    "      • is_qc_holiday       — Québec public holiday (CA-QC)\n",
    "      • is_jewish_holiday   — Israeli public/Jewish holiday (IL)\n",
    "    Optionally adds:\n",
    "      • qc_holiday_name\n",
    "      • jewish_holiday_name\n",
    "\n",
    "    Notes:\n",
    "      • Holiday checks are date-based (00:00–24:00 local calendar date),\n",
    "        not sundown-to-sundown observance.\n",
    "      • NaT timestamps are ignored gracefully.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1) Parse to datetime\n",
    "    out[ts_col] = pd.to_datetime(out[ts_col], errors=\"coerce\")\n",
    "\n",
    "    # 2) Get the calendar DATE to use for holiday lookup\n",
    "    #    - If tz-aware: convert to Montreal then take .date\n",
    "    #    - If naive: assume values already represent local Montreal wall-clock; just take .date\n",
    "    if getattr(out[ts_col].dt, \"tz\", None) is not None:\n",
    "        dates_for_calendar = out[ts_col].dt.tz_convert(local_tz).dt.date\n",
    "    else:\n",
    "        dates_for_calendar = out[ts_col].dt.date\n",
    "\n",
    "    # 3) Build a SAFE integer year range for the holiday objects\n",
    "    years_series = pd.Series(dates_for_calendar)\n",
    "    years_series = years_series.dropna().map(lambda d: int(pd.Timestamp(d).year))\n",
    "    if years_series.empty:\n",
    "        raise ValueError(\"No valid datetimes found to extract holiday years.\")\n",
    "    years = list(range(int(years_series.min()), int(years_series.max()) + 1))\n",
    "\n",
    "    # 4) Construct holiday calendars\n",
    "    qc_holidays = holidays.Canada(subdiv=\"QC\", years=years, observed=observed)\n",
    "    il_holidays = holidays.Israel(years=years, observed=observed)\n",
    "\n",
    "   # 5) Flag membership\n",
    "    out[\"is_qc_holiday\"] = [ (\"yes\" if d in qc_holidays else \"no\") if pd.notna(pd.Timestamp(d)) else \"no\"\n",
    "                             for d in dates_for_calendar ]\n",
    "    out[\"is_jewish_holiday\"] = [ (\"yes\" if d in il_holidays else \"no\") if pd.notna(pd.Timestamp(d)) else \"no\"\n",
    "                                 for d in dates_for_calendar ]\n",
    "\n",
    "    if include_names:\n",
    "        out[\"qc_holiday_name\"] = [ qc_holidays.get(d, \"no\") if pd.notna(pd.Timestamp(d)) else \"no\"\n",
    "                                   for d in dates_for_calendar ]\n",
    "        out[\"jewish_holiday_name\"] = [ il_holidays.get(d, \"no\") if pd.notna(pd.Timestamp(d)) else \"no\"\n",
    "                                       for d in dates_for_calendar ]\n",
    "\n",
    "    return out\n",
    "\n",
    "shift_types_dict = {'W1':'flow',\n",
    " 'X1':'pod',\n",
    " 'X3':'pod',\n",
    " 'X4':'vertical',\n",
    " 'X2':'vertical',\n",
    " 'WOC1':'oncall',\n",
    " 'WOC2':'oncall',\n",
    " 'WOC3':'oncall',\n",
    " 'X5':'pod',\n",
    " 'W3':'overlap',\n",
    " 'Y1':'pod',\n",
    " 'Y3':'pod',\n",
    " 'Y4':'vertical',\n",
    " 'Y2':'vertical',\n",
    " 'Y5':'pod',\n",
    " 'Z1':'night',\n",
    " 'Z2':'night',\n",
    " 'D1':'pod',\n",
    " 'R1':'pod',\n",
    " 'P1':'vertical',\n",
    " 'D2':'vertical',\n",
    " 'OC1':'oncall',\n",
    " 'OC2':'oncall',\n",
    " 'V1':'flow',\n",
    " 'A1':'pod',\n",
    " 'G1':'vertical',\n",
    " 'E1':'pod',\n",
    " 'R2':'pod',\n",
    " 'A2':'pod',\n",
    " 'P2':'vertical',\n",
    " 'E2':'vertical',\n",
    " 'N1':'night',\n",
    " 'N2':'night',\n",
    " 'L2':'overlap',\n",
    " 'L4':'overlap',\n",
    " 'H1':'teaching',\n",
    " 'B1':'vertical',\n",
    " 'L1':'overlap',\n",
    " 'W5':'overlap',\n",
    " 'L6':'overlap',\n",
    " 'B2':'vertical'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b029774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load hourly data\n",
    "df = pd.read_csv(\n",
    "    'https://www.dropbox.com/scl/fi/s83jig4zews1xz7vhezui/allDataWithCalculatedColumns.csv?rlkey=9mm4zwaugxyj2r4ooyd39y4nl&raw=1')\n",
    "df.ds = pd.to_datetime(df.ds, errors=\"coerce\")\n",
    "df['id'] = 'jgh'\n",
    "\n",
    "# Load shift data\n",
    "all_shifts_df = pd.read_csv('https://www.dropbox.com/scl/fi/yeyr2a7pj6nry8i2q3m0c/all_shifts.csv?rlkey=q1su2h8fqxfnlu7t1l2qe1w0q&raw=1')\n",
    "all_shifts_df['shift_start'] = pd.to_datetime(all_shifts_df['shift_start']).dt.round('h')\n",
    "all_shifts_df['shift_end'] = pd.to_datetime(all_shifts_df['shift_end']).dt.round('h')\n",
    "all_shifts_df['shift_type'] = all_shifts_df['shift_short_name'].map(shift_types_dict)\n",
    "\n",
    "# Create hourly rows\n",
    "# We'll use a list comprehension to generate the range for each row\n",
    "expanded_rows = []\n",
    "for _, row in all_shifts_df.iterrows():\n",
    "    # Create range. inclusive='left' means [start, end)\n",
    "    # If start == end (e.g. 0 length shift after rounding), it will be empty, which is correct\n",
    "    hours = pd.date_range(row['shift_start'], row['shift_end'], freq='h', inclusive='left')\n",
    "    for h in hours:\n",
    "        expanded_rows.append({\n",
    "            'ds': h,\n",
    "            'user': row['first_name']+row['last_name'],\n",
    "            'shift_type': row['shift_type'],\n",
    "            'shift_short_name': row['shift_short_name']\n",
    "        })\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Pivot\n",
    "# index=timestamp, columns=user_id, values=shift_type\n",
    "hourly_shifts_by_user_df = expanded_df.pivot_table(\n",
    "    index='ds', \n",
    "    columns='user', \n",
    "    values='shift_type', \n",
    "    aggfunc='first' # In case of duplicates, take the first\n",
    ")\n",
    "\n",
    "# Fill NaNs\n",
    "hourly_shifts_by_user_df = hourly_shifts_by_user_df.fillna('NotWorking')\n",
    "\n",
    "\n",
    "\n",
    "ID_COL = \"id\"\n",
    "TS_COL = \"ds\"\n",
    "TARGETS = ['total_tbs', 'Inflow_Total', 'overflow']\n",
    "\n",
    "df = df.copy()\n",
    "df[TS_COL] = pd.to_datetime(df[TS_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[TS_COL])\n",
    "\n",
    "# Snap to exact hours (lowercase 'h' to avoid FutureWarning)\n",
    "df[TS_COL] = df[TS_COL].dt.floor(\"h\")\n",
    "\n",
    "# Sort + dedupe\n",
    "df = df.sort_values([ID_COL, TS_COL]).drop_duplicates(\n",
    "    [ID_COL, TS_COL], keep=\"last\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Call apply with include_groups=False if supported; else fall back\n",
    "gb = df.groupby(ID_COL, group_keys=False)\n",
    "try:\n",
    "    df = gb.apply(regularize_hourly, include_groups=False)\n",
    "except TypeError:\n",
    "    # older pandas without include_groups\n",
    "    df = gb.apply(regularize_hourly)\n",
    "\n",
    "# Assert truly hourly (accept 'h' and 'H')\n",
    "g = df[df[ID_COL] == \"jgh\"].sort_values(TS_COL)\n",
    "freq = pd.infer_freq(g[TS_COL])\n",
    "if not freq:\n",
    "    raise ValueError(\"No inferable frequency after regularization.\")\n",
    "if to_offset(freq).name.lower() != \"h\":\n",
    "    # extra check independent of infer_freq\n",
    "    diffs = g[TS_COL].diff().dropna()\n",
    "    bad = g.loc[diffs != pd.Timedelta(hours=1), TS_COL].head(10).tolist()\n",
    "    raise ValueError(f\"Non-1h gaps remain around: {bad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2080324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting basic forecast\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ds</th>\n",
       "      <th>target_name</th>\n",
       "      <th>predictions</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jgh</td>\n",
       "      <td>2026-01-16 12:00:00</td>\n",
       "      <td>total_tbs</td>\n",
       "      <td>28.543800</td>\n",
       "      <td>23.212454</td>\n",
       "      <td>25.243147</td>\n",
       "      <td>26.575254</td>\n",
       "      <td>27.623253</td>\n",
       "      <td>28.543800</td>\n",
       "      <td>29.475594</td>\n",
       "      <td>30.508249</td>\n",
       "      <td>31.839729</td>\n",
       "      <td>33.883644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jgh</td>\n",
       "      <td>2026-01-16 13:00:00</td>\n",
       "      <td>total_tbs</td>\n",
       "      <td>33.836784</td>\n",
       "      <td>26.187853</td>\n",
       "      <td>28.956104</td>\n",
       "      <td>30.863159</td>\n",
       "      <td>32.399933</td>\n",
       "      <td>33.836784</td>\n",
       "      <td>35.220573</td>\n",
       "      <td>36.653198</td>\n",
       "      <td>38.350758</td>\n",
       "      <td>40.647682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jgh</td>\n",
       "      <td>2026-01-16 14:00:00</td>\n",
       "      <td>total_tbs</td>\n",
       "      <td>37.198807</td>\n",
       "      <td>27.709124</td>\n",
       "      <td>31.189690</td>\n",
       "      <td>33.578018</td>\n",
       "      <td>35.474442</td>\n",
       "      <td>37.198807</td>\n",
       "      <td>38.839813</td>\n",
       "      <td>40.515781</td>\n",
       "      <td>42.594368</td>\n",
       "      <td>45.361191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jgh</td>\n",
       "      <td>2026-01-16 15:00:00</td>\n",
       "      <td>total_tbs</td>\n",
       "      <td>40.522034</td>\n",
       "      <td>30.712509</td>\n",
       "      <td>34.379280</td>\n",
       "      <td>36.765217</td>\n",
       "      <td>38.740826</td>\n",
       "      <td>40.522034</td>\n",
       "      <td>42.446815</td>\n",
       "      <td>44.296211</td>\n",
       "      <td>46.670807</td>\n",
       "      <td>49.995560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jgh</td>\n",
       "      <td>2026-01-16 16:00:00</td>\n",
       "      <td>total_tbs</td>\n",
       "      <td>46.230476</td>\n",
       "      <td>36.014378</td>\n",
       "      <td>40.003166</td>\n",
       "      <td>42.613281</td>\n",
       "      <td>44.623528</td>\n",
       "      <td>46.230476</td>\n",
       "      <td>47.986023</td>\n",
       "      <td>50.023674</td>\n",
       "      <td>52.926353</td>\n",
       "      <td>56.454178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>jgh</td>\n",
       "      <td>2026-01-17 07:00:00</td>\n",
       "      <td>overflow</td>\n",
       "      <td>14.483986</td>\n",
       "      <td>9.219709</td>\n",
       "      <td>10.930721</td>\n",
       "      <td>12.205062</td>\n",
       "      <td>13.359463</td>\n",
       "      <td>14.483986</td>\n",
       "      <td>15.723854</td>\n",
       "      <td>17.030041</td>\n",
       "      <td>18.709183</td>\n",
       "      <td>21.035467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>jgh</td>\n",
       "      <td>2026-01-17 08:00:00</td>\n",
       "      <td>overflow</td>\n",
       "      <td>14.686199</td>\n",
       "      <td>9.257977</td>\n",
       "      <td>11.043676</td>\n",
       "      <td>12.371049</td>\n",
       "      <td>13.538076</td>\n",
       "      <td>14.686199</td>\n",
       "      <td>15.958594</td>\n",
       "      <td>17.390732</td>\n",
       "      <td>19.240765</td>\n",
       "      <td>21.753191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>jgh</td>\n",
       "      <td>2026-01-17 09:00:00</td>\n",
       "      <td>overflow</td>\n",
       "      <td>15.117300</td>\n",
       "      <td>9.290392</td>\n",
       "      <td>11.154719</td>\n",
       "      <td>12.571722</td>\n",
       "      <td>13.860757</td>\n",
       "      <td>15.117300</td>\n",
       "      <td>16.522289</td>\n",
       "      <td>18.020271</td>\n",
       "      <td>19.901325</td>\n",
       "      <td>22.554867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>jgh</td>\n",
       "      <td>2026-01-17 10:00:00</td>\n",
       "      <td>overflow</td>\n",
       "      <td>15.805276</td>\n",
       "      <td>9.475571</td>\n",
       "      <td>11.490541</td>\n",
       "      <td>13.062567</td>\n",
       "      <td>14.454515</td>\n",
       "      <td>15.805276</td>\n",
       "      <td>17.303015</td>\n",
       "      <td>18.944107</td>\n",
       "      <td>21.027637</td>\n",
       "      <td>23.895258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>jgh</td>\n",
       "      <td>2026-01-17 11:00:00</td>\n",
       "      <td>overflow</td>\n",
       "      <td>16.637947</td>\n",
       "      <td>9.558496</td>\n",
       "      <td>11.822174</td>\n",
       "      <td>13.572025</td>\n",
       "      <td>15.097269</td>\n",
       "      <td>16.637947</td>\n",
       "      <td>18.299660</td>\n",
       "      <td>20.082090</td>\n",
       "      <td>22.263573</td>\n",
       "      <td>25.208225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                  ds target_name  predictions        0.1        0.2  \\\n",
       "0   jgh 2026-01-16 12:00:00   total_tbs    28.543800  23.212454  25.243147   \n",
       "1   jgh 2026-01-16 13:00:00   total_tbs    33.836784  26.187853  28.956104   \n",
       "2   jgh 2026-01-16 14:00:00   total_tbs    37.198807  27.709124  31.189690   \n",
       "3   jgh 2026-01-16 15:00:00   total_tbs    40.522034  30.712509  34.379280   \n",
       "4   jgh 2026-01-16 16:00:00   total_tbs    46.230476  36.014378  40.003166   \n",
       "..  ...                 ...         ...          ...        ...        ...   \n",
       "67  jgh 2026-01-17 07:00:00    overflow    14.483986   9.219709  10.930721   \n",
       "68  jgh 2026-01-17 08:00:00    overflow    14.686199   9.257977  11.043676   \n",
       "69  jgh 2026-01-17 09:00:00    overflow    15.117300   9.290392  11.154719   \n",
       "70  jgh 2026-01-17 10:00:00    overflow    15.805276   9.475571  11.490541   \n",
       "71  jgh 2026-01-17 11:00:00    overflow    16.637947   9.558496  11.822174   \n",
       "\n",
       "          0.3        0.4        0.5        0.6        0.7        0.8  \\\n",
       "0   26.575254  27.623253  28.543800  29.475594  30.508249  31.839729   \n",
       "1   30.863159  32.399933  33.836784  35.220573  36.653198  38.350758   \n",
       "2   33.578018  35.474442  37.198807  38.839813  40.515781  42.594368   \n",
       "3   36.765217  38.740826  40.522034  42.446815  44.296211  46.670807   \n",
       "4   42.613281  44.623528  46.230476  47.986023  50.023674  52.926353   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "67  12.205062  13.359463  14.483986  15.723854  17.030041  18.709183   \n",
       "68  12.371049  13.538076  14.686199  15.958594  17.390732  19.240765   \n",
       "69  12.571722  13.860757  15.117300  16.522289  18.020271  19.901325   \n",
       "70  13.062567  14.454515  15.805276  17.303015  18.944107  21.027637   \n",
       "71  13.572025  15.097269  16.637947  18.299660  20.082090  22.263573   \n",
       "\n",
       "          0.9  \n",
       "0   33.883644  \n",
       "1   40.647682  \n",
       "2   45.361191  \n",
       "3   49.995560  \n",
       "4   56.454178  \n",
       "..        ...  \n",
       "67  21.035467  \n",
       "68  21.753191  \n",
       "69  22.554867  \n",
       "70  23.895258  \n",
       "71  25.208225  \n",
       "\n",
       "[72 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Predict\n",
    "print('Predicting basic forecast')\n",
    "basic_forecast = pipeline.predict_df(\n",
    "    df,\n",
    "    prediction_length=24,\n",
    "    # future_df = future_df.head(24),\n",
    "    # quantile_levels=[0.1, 0.5, 0.9],\n",
    "    # quantile_levels=[0.5],\n",
    "    id_column=ID_COL,\n",
    "    timestamp_column=TS_COL,\n",
    "    target=TARGETS,\n",
    ")\n",
    "\n",
    "basic_forecast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f6feb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting forecast with holidays\n",
      "Predicting forecast with staffing\n",
      "Predicting forecast with weather\n",
      "Predicting all variables forecast\n",
      "uploaded as b'chronos_forecast.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FileMetadata(client_modified=datetime.datetime(2026, 1, 16, 16, 28, 48), content_hash='c4ba1e53601cbb1947139e803a2050ecdb84563da4c1e696f5c2041cad6d144d', export_info=NOT_SET, file_lock_info=NOT_SET, has_explicit_shared_members=NOT_SET, id='id:oNSmVCFixyQAAAAAAABciA', is_downloadable=True, media_info=NOT_SET, name='chronos_forecast.csv', parent_shared_folder_id=NOT_SET, path_display='/chronos_forecast.csv', path_lower='/chronos_forecast.csv', preview_url=NOT_SET, property_groups=NOT_SET, rev='64883de9c74f17a19c0a3', server_modified=datetime.datetime(2026, 1, 16, 16, 31, 12), sharing_info=NOT_SET, size=5851, symlink_info=NOT_SET)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_with_holidays = add_holiday_flags(df, ts_col='ds', include_names=True)\n",
    "\n",
    "#create a dataframe with the next 24 hours timestamps hourly as column 'ds', with column 'id' jgh\n",
    "future_df = hourly_shifts_by_user_df.reset_index()[hourly_shifts_by_user_df.reset_index()['ds'] > df['ds'].max()]\n",
    "future_df['id'] = 'jgh'\n",
    "future_df = add_holiday_flags(future_df, ts_col='ds', include_names=True)\n",
    "\n",
    "# First, add holiday flags to future_df\n",
    "future_df_with_added_holidays = add_holiday_flags(future_df, ts_col='ds', include_names=True)\n",
    "\n",
    "# Then, select only the columns from future_df_with_added_holidays that are also in df_with_holidays\n",
    "common_columns = [col for col in future_df_with_added_holidays.columns if col in df_with_holidays.columns]\n",
    "future_df_with_holidays = future_df_with_added_holidays[common_columns]\n",
    "\n",
    "# Predict\n",
    "print('Predicting forecast with holidays')  \n",
    "forecast_with_holidays = pipeline.predict_df(\n",
    "    df_with_holidays,\n",
    "    prediction_length=24,\n",
    "    future_df = future_df_with_holidays.head(24),\n",
    "    # quantile_levels=[0.1, 0.5, 0.9],\n",
    "    # quantile_levels=[0.5],\n",
    "    id_column=ID_COL,\n",
    "    timestamp_column=TS_COL,\n",
    "    target=TARGETS,\n",
    ")\n",
    "\n",
    "# forecast_with_holidays\n",
    "\n",
    "\n",
    "df_with_staffing = df.merge(hourly_shifts_by_user_df, on='ds')\n",
    "future_df_with_staffing = hourly_shifts_by_user_df.reset_index()[hourly_shifts_by_user_df.reset_index()['ds'] > df['ds'].max()]\n",
    "future_df_with_staffing['id'] = 'jgh'\n",
    "\n",
    "print('Predicting forecast with staffing')\n",
    "forecast_with_staffing = pipeline.predict_df(\n",
    "    df_with_staffing,\n",
    "    prediction_length=24,\n",
    "    future_df = future_df_with_staffing.head(24),\n",
    "    # quantile_levels=[0.1, 0.5, 0.9],\n",
    "    # quantile_levels=[0.5],\n",
    "    id_column=ID_COL,\n",
    "    timestamp_column=TS_COL,\n",
    "    target=TARGETS,\n",
    ")\n",
    "\n",
    "# forecast_with_staffing\n",
    "\n",
    "weather_df = pd.read_csv('https://www.dropbox.com/scl/fi/gmhwwld9z9yychg4r0yuk/weather.csv?rlkey=66c78m90aviamr0x0uu72pfr8&raw=1')\n",
    "weather_df.ds = pd.to_datetime(weather_df.ds, errors=\"coerce\")\n",
    "\n",
    "\n",
    "future_weather_df = weather_df[weather_df.ds > df.ds.max()].head(24)\n",
    "future_weather_df['id']='jgh'\n",
    "\n",
    "print('Predicting forecast with weather')\n",
    "# Predict\n",
    "forecast_with_weather = pipeline.predict_df(\n",
    "    #join df with weather_df on ds\n",
    "    df.merge(weather_df, on='ds'),\n",
    "    prediction_length=24,\n",
    "    #weather_df where ds is greater than the max of df.ds.max()\n",
    "    future_df = future_weather_df,\n",
    "    # future_df = future_df.head(24),\n",
    "    # quantile_levels=[0.1, 0.5, 0.9],\n",
    "    quantile_levels=[0.5],\n",
    "    id_column=ID_COL,\n",
    "    timestamp_column=TS_COL,\n",
    "    target=TARGETS,\n",
    ")\n",
    "\n",
    "#forecast_with_weather\n",
    "\n",
    "\n",
    "# All variables forecast without future\n",
    "# all_variable_df = add_holiday_flags(df_with_staffing, ts_col='ds', include_names=True).merge(weather_df, on='ds')\n",
    "# print('Predicting all variables forecast without future')\n",
    "# forecast_all_vars_without_future = pipeline.predict_df(\n",
    "#     all_variable_df,\n",
    "#     prediction_length=24,\n",
    "#     # quantile_levels=[0.1, 0.5, 0.9],\n",
    "#     quantile_levels=[0.5],\n",
    "#     id_column=ID_COL,\n",
    "#     timestamp_column=TS_COL,\n",
    "#     target=TARGETS,\n",
    "# )\n",
    "\n",
    "#forecast_all_vars_without_future\n",
    "\n",
    "# All variables forecast\n",
    "print('Predicting all variables forecast')\n",
    "all_variable_df = add_holiday_flags(df_with_staffing, ts_col='ds', include_names=True).merge(weather_df, on='ds')\n",
    "\n",
    "forecast_all_vars_with_future = pipeline.predict_df(\n",
    "    all_variable_df,\n",
    "    prediction_length=24,\n",
    "    #future_df should be future_df_with_staffing merged with future_weather_df on 'ds' and 'id'\n",
    "    future_df = future_df_with_staffing.merge(future_weather_df, on=['ds', 'id']),\n",
    "    # quantile_levels=[0.1, 0.5, 0.9],\n",
    "    quantile_levels=[0.5],\n",
    "    id_column=ID_COL,\n",
    "    timestamp_column=TS_COL,\n",
    "    target=TARGETS,\n",
    ")   \n",
    "\n",
    "#forecast_all_vars_with_future\n",
    "\n",
    "#join the predictions columns of basic_forecast, forecast_with_holidays, forecast_with_staffing, forecast_with_weather, forecast_all_vars_without_future, forecast_all_vars_with_future on the 'ds' column\n",
    "basic_forecast = basic_forecast[['ds', 'target_name', 'predictions']].rename(columns={'predictions':'basic_forecast'})\n",
    "forecast_with_holidays = forecast_with_holidays[['ds', 'target_name', 'predictions']].rename(columns={'predictions':'forecast_with_holidays'})\n",
    "forecast_with_staffing = forecast_with_staffing[['ds', 'target_name', 'predictions']].rename(columns={'predictions':'forecast_with_staffing'})\n",
    "forecast_with_weather = forecast_with_weather[['ds', 'target_name', 'predictions']].rename(columns={'predictions':'forecast_with_weather'})\n",
    "# forecast_all_vars_without_future = forecast_all_vars_without_future[['ds', 'target_name', 'predictions']].rename(columns={'predictions':'forecast_all_vars_without_future'})\n",
    "forecast_all_vars_with_future = forecast_all_vars_with_future[['ds', 'target_name', 'predictions']].rename(columns={'predictions':'forecast_all_vars_with_future'})\n",
    "\n",
    "pred_df = basic_forecast.merge(forecast_with_holidays, on=['ds', 'target_name']).merge(forecast_with_staffing, on=['ds', 'target_name']).merge(forecast_with_weather, on=['ds', 'target_name']).merge(forecast_all_vars_with_future, on=['ds', 'target_name'])\n",
    "pred_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df = df.merge(hourly_shifts_by_user_df, on='ds')\n",
    "# df = add_holiday_flags(df, ts_col='ds', include_names=True)\n",
    "\n",
    "# future_df = hourly_shifts_by_user_df.reset_index()[hourly_shifts_by_user_df.reset_index()['ds'] > df['ds'].max()]\n",
    "# future_df['id'] = 'jgh'\n",
    "# future_df = add_holiday_flags(future_df, ts_col='ds', include_names=True)\n",
    "\n",
    "\n",
    "pred_df.to_csv('chronos_forecast.csv', index=False)\n",
    "\n",
    "dropbox_app_key = os.environ.get(\"DROPBOX_APP_KEY\")\n",
    "dropbox_app_secret = os.environ.get(\"DROPBOX_APP_SECRET\")\n",
    "dropbox_refresh_token = os.environ.get(\"DROPBOX_REFRESH_TOKEN\")\n",
    "\n",
    "# exchange the authorization code for an access token:\n",
    "token_url = \"https://api.dropboxapi.com/oauth2/token\"\n",
    "params = {\n",
    "    \"grant_type\": \"refresh_token\",\n",
    "    \"refresh_token\": dropbox_refresh_token,\n",
    "    \"client_id\": dropbox_app_key,\n",
    "    \"client_secret\": dropbox_app_secret\n",
    "}\n",
    "r = requests.post(token_url, data=params)\n",
    "\n",
    "dropbox_access_token = r.json()['access_token']\n",
    "\n",
    "dbx = dropbox.Dropbox(dropbox_access_token)\n",
    "\n",
    "upload(dbx, 'chronos_forecast.csv', '', '',\n",
    "            'chronos_forecast.csv', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74113130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ed-flow-2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
